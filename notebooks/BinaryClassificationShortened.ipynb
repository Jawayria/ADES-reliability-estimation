{
 "cells": [
  {
   "cell_type": "code",
   "id": "20876ddc86219ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.386378Z",
     "start_time": "2025-02-12T18:18:50.377901Z"
    }
   },
   "source": [
    "import importlib\n",
    "import datamanip\n",
    "importlib.reload(datamanip)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datamanip' (namespace) from ['C:\\\\Users\\\\pavel\\\\PycharmProjects\\\\ADES-reliability\\\\src\\\\datamanip']>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "93f8ba8bac98d962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.407738Z",
     "start_time": "2025-02-12T18:18:50.402954Z"
    }
   },
   "source": [
    "import torch\n",
    "from filepath import *\n",
    "from datamanip.plots import generate_matrix, generate_metrics\n",
    "from models.GAT import GAT#, GCN, SimpleMPNN\n",
    "from train_eval.train import train\n",
    "from train_eval.evaluate import evaluate\n",
    "from datamanip.datasetmanip.three_five_dataset import ThreeFiveDataset\n",
    "from datamanip.datasetmanip.dataset_util import create_loaders"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "a2ed7ad0113530ba",
   "metadata": {},
   "source": [
    "# Model Selection by number of 9's\n",
    "### 1. Match 1-1: 0 vs. 7\n",
    "### 2. Match 2-1: 0/1 vs. 2/3\n",
    "### 3. Match 2-2: 4/5 vs. 6/7\n",
    "### 4. Match 3-1: 0 vs. 1\n",
    "### 5. Match 3-2: 2 vs. 3\n",
    "### 6. Match 3-3: 4 vs. 5\n",
    "### 7. Match 3-4: 6 vs. 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d77e32da00081ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.424521Z",
     "start_time": "2025-02-12T18:18:50.420417Z"
    }
   },
   "source": "match = \"3-4\"",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "bf2f6a33f2886a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.454137Z",
     "start_time": "2025-02-12T18:18:50.446066Z"
    }
   },
   "source": [
    "if match == \"1-1\":\n",
    "    NUM_EPOCHS = 200\n",
    "    DROPOUT_RATE = 0.15\n",
    "    LEARNING_RATE = 0.0008\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 30\n",
    "    HIDDEN_DIM = 128\n",
    "elif match == \"2-1\":\n",
    "    NUM_EPOCHS = 200\n",
    "    DROPOUT_RATE = 0.15\n",
    "    LEARNING_RATE = 0.0008\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 15\n",
    "    HIDDEN_DIM = 16\n",
    "elif match == \"2-2\":\n",
    "    NUM_EPOCHS = 150\n",
    "    DROPOUT_RATE = 0.3\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 15\n",
    "    HIDDEN_DIM = 28\n",
    "elif match == \"3-1\":\n",
    "    NUM_EPOCHS = 150\n",
    "    DROPOUT_RATE = 0.3\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 15\n",
    "    HIDDEN_DIM = 82\n",
    "elif match == \"3-2\":\n",
    "    NUM_EPOCHS = 200\n",
    "    DROPOUT_RATE = 0.3\n",
    "    LEARNING_RATE = 0.0001\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 30\n",
    "    HIDDEN_DIM = 128\n",
    "elif match == \"3-3\":\n",
    "    NUM_EPOCHS = 250\n",
    "    DROPOUT_RATE = 0.3\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 30\n",
    "    HIDDEN_DIM = 64\n",
    "elif match == \"3-4\":\n",
    "    NUM_EPOCHS = 200\n",
    "    DROPOUT_RATE = 0.2\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 20\n",
    "    HIDDEN_DIM = 128\n",
    "else:\n",
    "    NUM_EPOCHS = 100\n",
    "    DROPOUT_RATE = 0.2\n",
    "    LEARNING_RATE = 0.0007\n",
    "    NODE_FEATURES = 12\n",
    "    PATIENCE = 15\n",
    "    HIDDEN_DIM = 64"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "2ba9a1fa93b8f7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.497891Z",
     "start_time": "2025-02-12T18:18:50.462193Z"
    }
   },
   "source": [
    "#Create list of Data objects, each containing the node features, edge indices, and target values\n",
    "train_list = ThreeFiveDataset(root=dataset_path, match=match, test_train_val=\"train\")\n",
    "test_list = ThreeFiveDataset(root=dataset_path, match=match, test_train_val=\"test\")\n",
    "val_list = ThreeFiveDataset(root=dataset_path, match=match, test_train_val=\"val\")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "c4df5f6e44f44d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.562391Z",
     "start_time": "2025-02-12T18:18:50.516452Z"
    }
   },
   "source": [
    "train_loader, val_loader, test_loader = create_loaders(train_list, val_list, test_list)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "bb6e9a3752579873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.585310Z",
     "start_time": "2025-02-12T18:18:50.576340Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT(input_dim=NODE_FEATURES, hidden_dim=HIDDEN_DIM, output_dim=2, dropout_rate=DROPOUT_RATE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "558a8e6533c23b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:18:50.603213Z",
     "start_time": "2025-02-12T18:18:50.596754Z"
    }
   },
   "source": [
    "train_config = {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"device\": device,\n",
    "    \"criterion\": criterion,\n",
    "    \"optimizer\": optimizer,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "e718e13b7de516fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:22.059323Z",
     "start_time": "2025-02-12T18:18:50.616353Z"
    }
   },
   "source": [
    "train(model, train_loader, val_loader, train_config, model_checkpoints_path + \"/booster/\" + match + \".pth\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 0.3141\n",
      "Epoch 1/200, Validation Loss: 0.2352\n",
      "Best model updated based on validation loss.\n",
      "Epoch 2/200, Train Loss: 0.2824\n",
      "Epoch 2/200, Validation Loss: 0.2245\n",
      "Best model updated based on validation loss.\n",
      "Epoch 3/200, Train Loss: 0.2771\n",
      "Epoch 3/200, Validation Loss: 0.2277\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 4/200, Train Loss: 0.2712\n",
      "Epoch 4/200, Validation Loss: 0.2088\n",
      "Best model updated based on validation loss.\n",
      "Epoch 5/200, Train Loss: 0.2727\n",
      "Epoch 5/200, Validation Loss: 0.2138\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 6/200, Train Loss: 0.2683\n",
      "Epoch 6/200, Validation Loss: 0.2455\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 7/200, Train Loss: 0.2599\n",
      "Epoch 7/200, Validation Loss: 0.2248\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 8/200, Train Loss: 0.2607\n",
      "Epoch 8/200, Validation Loss: 0.2144\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 9/200, Train Loss: 0.2635\n",
      "Epoch 9/200, Validation Loss: 0.2117\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 10/200, Train Loss: 0.2602\n",
      "Epoch 10/200, Validation Loss: 0.2206\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 11/200, Train Loss: 0.2586\n",
      "Epoch 11/200, Validation Loss: 0.2155\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 12/200, Train Loss: 0.2611\n",
      "Epoch 12/200, Validation Loss: 0.3261\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 13/200, Train Loss: 0.2577\n",
      "Epoch 13/200, Validation Loss: 0.2102\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 14/200, Train Loss: 0.2577\n",
      "Epoch 14/200, Validation Loss: 0.2117\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 15/200, Train Loss: 0.2540\n",
      "Epoch 15/200, Validation Loss: 0.2019\n",
      "Best model updated based on validation loss.\n",
      "Epoch 16/200, Train Loss: 0.2554\n",
      "Epoch 16/200, Validation Loss: 0.2532\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 17/200, Train Loss: 0.2516\n",
      "Epoch 17/200, Validation Loss: 0.2033\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 18/200, Train Loss: 0.2574\n",
      "Epoch 18/200, Validation Loss: 0.1979\n",
      "Best model updated based on validation loss.\n",
      "Epoch 19/200, Train Loss: 0.2500\n",
      "Epoch 19/200, Validation Loss: 0.2103\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 20/200, Train Loss: 0.2506\n",
      "Epoch 20/200, Validation Loss: 0.2228\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 21/200, Train Loss: 0.2459\n",
      "Epoch 21/200, Validation Loss: 0.2063\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 22/200, Train Loss: 0.2514\n",
      "Epoch 22/200, Validation Loss: 0.1967\n",
      "Best model updated based on validation loss.\n",
      "Epoch 23/200, Train Loss: 0.2457\n",
      "Epoch 23/200, Validation Loss: 0.2016\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 24/200, Train Loss: 0.2490\n",
      "Epoch 24/200, Validation Loss: 0.2050\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 25/200, Train Loss: 0.2491\n",
      "Epoch 25/200, Validation Loss: 0.1933\n",
      "Best model updated based on validation loss.\n",
      "Epoch 26/200, Train Loss: 0.2470\n",
      "Epoch 26/200, Validation Loss: 0.2205\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 27/200, Train Loss: 0.2469\n",
      "Epoch 27/200, Validation Loss: 0.2262\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 28/200, Train Loss: 0.2450\n",
      "Epoch 28/200, Validation Loss: 0.2077\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 29/200, Train Loss: 0.2450\n",
      "Epoch 29/200, Validation Loss: 0.2058\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 30/200, Train Loss: 0.2468\n",
      "Epoch 30/200, Validation Loss: 0.2265\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 31/200, Train Loss: 0.2369\n",
      "Epoch 31/200, Validation Loss: 0.2427\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 32/200, Train Loss: 0.2436\n",
      "Epoch 32/200, Validation Loss: 0.2008\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 33/200, Train Loss: 0.2379\n",
      "Epoch 33/200, Validation Loss: 0.2120\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 34/200, Train Loss: 0.2412\n",
      "Epoch 34/200, Validation Loss: 0.1904\n",
      "Best model updated based on validation loss.\n",
      "Epoch 35/200, Train Loss: 0.2351\n",
      "Epoch 35/200, Validation Loss: 0.1834\n",
      "Best model updated based on validation loss.\n",
      "Epoch 36/200, Train Loss: 0.2390\n",
      "Epoch 36/200, Validation Loss: 0.2538\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 37/200, Train Loss: 0.2388\n",
      "Epoch 37/200, Validation Loss: 0.1835\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 38/200, Train Loss: 0.2335\n",
      "Epoch 38/200, Validation Loss: 0.1860\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 39/200, Train Loss: 0.2397\n",
      "Epoch 39/200, Validation Loss: 0.1972\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 40/200, Train Loss: 0.2281\n",
      "Epoch 40/200, Validation Loss: 0.1982\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 41/200, Train Loss: 0.2326\n",
      "Epoch 41/200, Validation Loss: 0.1814\n",
      "Best model updated based on validation loss.\n",
      "Epoch 42/200, Train Loss: 0.2379\n",
      "Epoch 42/200, Validation Loss: 0.1976\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 43/200, Train Loss: 0.2322\n",
      "Epoch 43/200, Validation Loss: 0.1819\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 44/200, Train Loss: 0.2292\n",
      "Epoch 44/200, Validation Loss: 0.1871\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 45/200, Train Loss: 0.2365\n",
      "Epoch 45/200, Validation Loss: 0.1834\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 46/200, Train Loss: 0.2310\n",
      "Epoch 46/200, Validation Loss: 0.2126\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 47/200, Train Loss: 0.2326\n",
      "Epoch 47/200, Validation Loss: 0.2800\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 48/200, Train Loss: 0.2315\n",
      "Epoch 48/200, Validation Loss: 0.1772\n",
      "Best model updated based on validation loss.\n",
      "Epoch 49/200, Train Loss: 0.2233\n",
      "Epoch 49/200, Validation Loss: 0.1816\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 50/200, Train Loss: 0.2297\n",
      "Epoch 50/200, Validation Loss: 0.1745\n",
      "Best model updated based on validation loss.\n",
      "Epoch 51/200, Train Loss: 0.2276\n",
      "Epoch 51/200, Validation Loss: 0.1741\n",
      "Best model updated based on validation loss.\n",
      "Epoch 52/200, Train Loss: 0.2229\n",
      "Epoch 52/200, Validation Loss: 0.1758\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 53/200, Train Loss: 0.2254\n",
      "Epoch 53/200, Validation Loss: 0.1752\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 54/200, Train Loss: 0.2188\n",
      "Epoch 54/200, Validation Loss: 0.1694\n",
      "Best model updated based on validation loss.\n",
      "Epoch 55/200, Train Loss: 0.2132\n",
      "Epoch 55/200, Validation Loss: 0.1700\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 56/200, Train Loss: 0.2164\n",
      "Epoch 56/200, Validation Loss: 0.1724\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 57/200, Train Loss: 0.2189\n",
      "Epoch 57/200, Validation Loss: 0.2218\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 58/200, Train Loss: 0.2165\n",
      "Epoch 58/200, Validation Loss: 0.1703\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 59/200, Train Loss: 0.2191\n",
      "Epoch 59/200, Validation Loss: 0.2258\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 60/200, Train Loss: 0.2195\n",
      "Epoch 60/200, Validation Loss: 0.1800\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 61/200, Train Loss: 0.2231\n",
      "Epoch 61/200, Validation Loss: 0.1822\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 62/200, Train Loss: 0.2161\n",
      "Epoch 62/200, Validation Loss: 0.2541\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 63/200, Train Loss: 0.2155\n",
      "Epoch 63/200, Validation Loss: 0.1614\n",
      "Best model updated based on validation loss.\n",
      "Epoch 64/200, Train Loss: 0.2127\n",
      "Epoch 64/200, Validation Loss: 0.1603\n",
      "Best model updated based on validation loss.\n",
      "Epoch 65/200, Train Loss: 0.2064\n",
      "Epoch 65/200, Validation Loss: 0.1630\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 66/200, Train Loss: 0.2068\n",
      "Epoch 66/200, Validation Loss: 0.1540\n",
      "Best model updated based on validation loss.\n",
      "Epoch 67/200, Train Loss: 0.2081\n",
      "Epoch 67/200, Validation Loss: 0.1657\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 68/200, Train Loss: 0.2086\n",
      "Epoch 68/200, Validation Loss: 0.1697\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 69/200, Train Loss: 0.2071\n",
      "Epoch 69/200, Validation Loss: 0.1611\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 70/200, Train Loss: 0.2114\n",
      "Epoch 70/200, Validation Loss: 0.2592\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 71/200, Train Loss: 0.2070\n",
      "Epoch 71/200, Validation Loss: 0.1547\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 72/200, Train Loss: 0.2079\n",
      "Epoch 72/200, Validation Loss: 0.1866\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 73/200, Train Loss: 0.2052\n",
      "Epoch 73/200, Validation Loss: 0.1542\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 74/200, Train Loss: 0.2334\n",
      "Epoch 74/200, Validation Loss: 0.1706\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 75/200, Train Loss: 0.2154\n",
      "Epoch 75/200, Validation Loss: 0.1743\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 76/200, Train Loss: 0.2176\n",
      "Epoch 76/200, Validation Loss: 0.1696\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 77/200, Train Loss: 0.2153\n",
      "Epoch 77/200, Validation Loss: 0.1940\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch 78/200, Train Loss: 0.2154\n",
      "Epoch 78/200, Validation Loss: 0.1621\n",
      "No improvement in validation loss for 12 epoch(s).\n",
      "Epoch 79/200, Train Loss: 0.2090\n",
      "Epoch 79/200, Validation Loss: 0.1592\n",
      "No improvement in validation loss for 13 epoch(s).\n",
      "Epoch 80/200, Train Loss: 0.2131\n",
      "Epoch 80/200, Validation Loss: 0.1809\n",
      "No improvement in validation loss for 14 epoch(s).\n",
      "Epoch 81/200, Train Loss: 0.2069\n",
      "Epoch 81/200, Validation Loss: 0.1617\n",
      "No improvement in validation loss for 15 epoch(s).\n",
      "Epoch 82/200, Train Loss: 0.2016\n",
      "Epoch 82/200, Validation Loss: 0.1593\n",
      "No improvement in validation loss for 16 epoch(s).\n",
      "Epoch 83/200, Train Loss: 0.2050\n",
      "Epoch 83/200, Validation Loss: 0.1503\n",
      "Best model updated based on validation loss.\n",
      "Epoch 84/200, Train Loss: 0.2054\n",
      "Epoch 84/200, Validation Loss: 0.1644\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 85/200, Train Loss: 0.2069\n",
      "Epoch 85/200, Validation Loss: 0.1670\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 86/200, Train Loss: 0.2075\n",
      "Epoch 86/200, Validation Loss: 0.1556\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 87/200, Train Loss: 0.2001\n",
      "Epoch 87/200, Validation Loss: 0.1576\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 88/200, Train Loss: 0.2059\n",
      "Epoch 88/200, Validation Loss: 0.1725\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 89/200, Train Loss: 0.2007\n",
      "Epoch 89/200, Validation Loss: 0.1445\n",
      "Best model updated based on validation loss.\n",
      "Epoch 90/200, Train Loss: 0.1985\n",
      "Epoch 90/200, Validation Loss: 0.1480\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 91/200, Train Loss: 0.2041\n",
      "Epoch 91/200, Validation Loss: 0.1436\n",
      "Best model updated based on validation loss.\n",
      "Epoch 92/200, Train Loss: 0.1967\n",
      "Epoch 92/200, Validation Loss: 0.1557\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 93/200, Train Loss: 0.2041\n",
      "Epoch 93/200, Validation Loss: 0.1606\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 94/200, Train Loss: 0.2004\n",
      "Epoch 94/200, Validation Loss: 0.1567\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 95/200, Train Loss: 0.1999\n",
      "Epoch 95/200, Validation Loss: 0.1514\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 96/200, Train Loss: 0.2014\n",
      "Epoch 96/200, Validation Loss: 0.1419\n",
      "Best model updated based on validation loss.\n",
      "Epoch 97/200, Train Loss: 0.1979\n",
      "Epoch 97/200, Validation Loss: 0.1440\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 98/200, Train Loss: 0.2034\n",
      "Epoch 98/200, Validation Loss: 0.1436\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 99/200, Train Loss: 0.1982\n",
      "Epoch 99/200, Validation Loss: 0.1436\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 100/200, Train Loss: 0.1904\n",
      "Epoch 100/200, Validation Loss: 0.1468\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 101/200, Train Loss: 0.1976\n",
      "Epoch 101/200, Validation Loss: 0.1462\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 102/200, Train Loss: 0.1976\n",
      "Epoch 102/200, Validation Loss: 0.1385\n",
      "Best model updated based on validation loss.\n",
      "Epoch 103/200, Train Loss: 0.1933\n",
      "Epoch 103/200, Validation Loss: 0.1708\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 104/200, Train Loss: 0.1943\n",
      "Epoch 104/200, Validation Loss: 0.1392\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 105/200, Train Loss: 0.1930\n",
      "Epoch 105/200, Validation Loss: 0.1542\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 106/200, Train Loss: 0.1990\n",
      "Epoch 106/200, Validation Loss: 0.1429\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 107/200, Train Loss: 0.1930\n",
      "Epoch 107/200, Validation Loss: 0.1469\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 108/200, Train Loss: 0.1968\n",
      "Epoch 108/200, Validation Loss: 0.1497\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 109/200, Train Loss: 0.1961\n",
      "Epoch 109/200, Validation Loss: 0.1630\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 110/200, Train Loss: 0.1894\n",
      "Epoch 110/200, Validation Loss: 0.1990\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 111/200, Train Loss: 0.1933\n",
      "Epoch 111/200, Validation Loss: 0.1421\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 112/200, Train Loss: 0.2009\n",
      "Epoch 112/200, Validation Loss: 0.1437\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 113/200, Train Loss: 0.1965\n",
      "Epoch 113/200, Validation Loss: 0.1385\n",
      "Best model updated based on validation loss.\n",
      "Epoch 114/200, Train Loss: 0.1836\n",
      "Epoch 114/200, Validation Loss: 0.1401\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 115/200, Train Loss: 0.1975\n",
      "Epoch 115/200, Validation Loss: 0.1398\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 116/200, Train Loss: 0.1943\n",
      "Epoch 116/200, Validation Loss: 0.1370\n",
      "Best model updated based on validation loss.\n",
      "Epoch 117/200, Train Loss: 0.1998\n",
      "Epoch 117/200, Validation Loss: 0.1750\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 118/200, Train Loss: 0.1993\n",
      "Epoch 118/200, Validation Loss: 0.1306\n",
      "Best model updated based on validation loss.\n",
      "Epoch 119/200, Train Loss: 0.1884\n",
      "Epoch 119/200, Validation Loss: 0.1345\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 120/200, Train Loss: 0.1874\n",
      "Epoch 120/200, Validation Loss: 0.1509\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 121/200, Train Loss: 0.1924\n",
      "Epoch 121/200, Validation Loss: 0.1705\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 122/200, Train Loss: 0.1969\n",
      "Epoch 122/200, Validation Loss: 0.1447\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 123/200, Train Loss: 0.1956\n",
      "Epoch 123/200, Validation Loss: 0.1442\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 124/200, Train Loss: 0.1992\n",
      "Epoch 124/200, Validation Loss: 0.1445\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 125/200, Train Loss: 0.1899\n",
      "Epoch 125/200, Validation Loss: 0.1910\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 126/200, Train Loss: 0.1942\n",
      "Epoch 126/200, Validation Loss: 0.1435\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 127/200, Train Loss: 0.1894\n",
      "Epoch 127/200, Validation Loss: 0.1452\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 128/200, Train Loss: 0.1943\n",
      "Epoch 128/200, Validation Loss: 0.1419\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 129/200, Train Loss: 0.1890\n",
      "Epoch 129/200, Validation Loss: 0.1376\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch 130/200, Train Loss: 0.1894\n",
      "Epoch 130/200, Validation Loss: 0.1498\n",
      "No improvement in validation loss for 12 epoch(s).\n",
      "Epoch 131/200, Train Loss: 0.1871\n",
      "Epoch 131/200, Validation Loss: 0.1624\n",
      "No improvement in validation loss for 13 epoch(s).\n",
      "Epoch 132/200, Train Loss: 0.1815\n",
      "Epoch 132/200, Validation Loss: 0.1292\n",
      "Best model updated based on validation loss.\n",
      "Epoch 133/200, Train Loss: 0.1902\n",
      "Epoch 133/200, Validation Loss: 0.1425\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 134/200, Train Loss: 0.1910\n",
      "Epoch 134/200, Validation Loss: 0.1359\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 135/200, Train Loss: 0.1904\n",
      "Epoch 135/200, Validation Loss: 0.1391\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 136/200, Train Loss: 0.1888\n",
      "Epoch 136/200, Validation Loss: 0.1367\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 137/200, Train Loss: 0.1825\n",
      "Epoch 137/200, Validation Loss: 0.1321\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 138/200, Train Loss: 0.1887\n",
      "Epoch 138/200, Validation Loss: 0.1474\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 139/200, Train Loss: 0.1920\n",
      "Epoch 139/200, Validation Loss: 0.1431\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 140/200, Train Loss: 0.1869\n",
      "Epoch 140/200, Validation Loss: 0.1356\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 141/200, Train Loss: 0.1880\n",
      "Epoch 141/200, Validation Loss: 0.1508\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 142/200, Train Loss: 0.1837\n",
      "Epoch 142/200, Validation Loss: 0.1438\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 143/200, Train Loss: 0.1961\n",
      "Epoch 143/200, Validation Loss: 0.1314\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch 144/200, Train Loss: 0.1872\n",
      "Epoch 144/200, Validation Loss: 0.1439\n",
      "No improvement in validation loss for 12 epoch(s).\n",
      "Epoch 145/200, Train Loss: 0.1828\n",
      "Epoch 145/200, Validation Loss: 0.1314\n",
      "No improvement in validation loss for 13 epoch(s).\n",
      "Epoch 146/200, Train Loss: 0.1829\n",
      "Epoch 146/200, Validation Loss: 0.1418\n",
      "No improvement in validation loss for 14 epoch(s).\n",
      "Epoch 147/200, Train Loss: 0.1878\n",
      "Epoch 147/200, Validation Loss: 0.1384\n",
      "No improvement in validation loss for 15 epoch(s).\n",
      "Epoch 148/200, Train Loss: 0.1864\n",
      "Epoch 148/200, Validation Loss: 0.1317\n",
      "No improvement in validation loss for 16 epoch(s).\n",
      "Epoch 149/200, Train Loss: 0.1835\n",
      "Epoch 149/200, Validation Loss: 0.1499\n",
      "No improvement in validation loss for 17 epoch(s).\n",
      "Epoch 150/200, Train Loss: 0.1835\n",
      "Epoch 150/200, Validation Loss: 0.1291\n",
      "Best model updated based on validation loss.\n",
      "Epoch 151/200, Train Loss: 0.1852\n",
      "Epoch 151/200, Validation Loss: 0.1340\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 152/200, Train Loss: 0.1842\n",
      "Epoch 152/200, Validation Loss: 0.1322\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 153/200, Train Loss: 0.1823\n",
      "Epoch 153/200, Validation Loss: 0.1302\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 154/200, Train Loss: 0.1837\n",
      "Epoch 154/200, Validation Loss: 0.1277\n",
      "Best model updated based on validation loss.\n",
      "Epoch 155/200, Train Loss: 0.1866\n",
      "Epoch 155/200, Validation Loss: 0.1458\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 156/200, Train Loss: 0.1908\n",
      "Epoch 156/200, Validation Loss: 0.1333\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 157/200, Train Loss: 0.1803\n",
      "Epoch 157/200, Validation Loss: 0.1275\n",
      "Best model updated based on validation loss.\n",
      "Epoch 158/200, Train Loss: 0.1849\n",
      "Epoch 158/200, Validation Loss: 0.1388\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 159/200, Train Loss: 0.1828\n",
      "Epoch 159/200, Validation Loss: 0.1317\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 160/200, Train Loss: 0.1817\n",
      "Epoch 160/200, Validation Loss: 0.1274\n",
      "Best model updated based on validation loss.\n",
      "Epoch 161/200, Train Loss: 0.1913\n",
      "Epoch 161/200, Validation Loss: 0.1293\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 162/200, Train Loss: 0.1836\n",
      "Epoch 162/200, Validation Loss: 0.1666\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 163/200, Train Loss: 0.1890\n",
      "Epoch 163/200, Validation Loss: 0.1371\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 164/200, Train Loss: 0.1853\n",
      "Epoch 164/200, Validation Loss: 0.1372\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 165/200, Train Loss: 0.1801\n",
      "Epoch 165/200, Validation Loss: 0.1413\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 166/200, Train Loss: 0.1975\n",
      "Epoch 166/200, Validation Loss: 0.1273\n",
      "Best model updated based on validation loss.\n",
      "Epoch 167/200, Train Loss: 0.1789\n",
      "Epoch 167/200, Validation Loss: 0.1276\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 168/200, Train Loss: 0.1736\n",
      "Epoch 168/200, Validation Loss: 0.1510\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 169/200, Train Loss: 0.1799\n",
      "Epoch 169/200, Validation Loss: 0.1491\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 170/200, Train Loss: 0.1806\n",
      "Epoch 170/200, Validation Loss: 0.1898\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 171/200, Train Loss: 0.1797\n",
      "Epoch 171/200, Validation Loss: 0.1574\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 172/200, Train Loss: 0.1831\n",
      "Epoch 172/200, Validation Loss: 0.1372\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 173/200, Train Loss: 0.1840\n",
      "Epoch 173/200, Validation Loss: 0.1415\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 174/200, Train Loss: 0.1850\n",
      "Epoch 174/200, Validation Loss: 0.1607\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 175/200, Train Loss: 0.1847\n",
      "Epoch 175/200, Validation Loss: 0.1349\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 176/200, Train Loss: 0.1800\n",
      "Epoch 176/200, Validation Loss: 0.1489\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 177/200, Train Loss: 0.1854\n",
      "Epoch 177/200, Validation Loss: 0.1435\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch 178/200, Train Loss: 0.1777\n",
      "Epoch 178/200, Validation Loss: 0.1241\n",
      "Best model updated based on validation loss.\n",
      "Epoch 179/200, Train Loss: 0.1834\n",
      "Epoch 179/200, Validation Loss: 0.1474\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 180/200, Train Loss: 0.1812\n",
      "Epoch 180/200, Validation Loss: 0.1302\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 181/200, Train Loss: 0.1770\n",
      "Epoch 181/200, Validation Loss: 0.1278\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 182/200, Train Loss: 0.1822\n",
      "Epoch 182/200, Validation Loss: 0.1409\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 183/200, Train Loss: 0.1806\n",
      "Epoch 183/200, Validation Loss: 0.1450\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 184/200, Train Loss: 0.1770\n",
      "Epoch 184/200, Validation Loss: 0.1254\n",
      "No improvement in validation loss for 6 epoch(s).\n",
      "Epoch 185/200, Train Loss: 0.1724\n",
      "Epoch 185/200, Validation Loss: 0.1412\n",
      "No improvement in validation loss for 7 epoch(s).\n",
      "Epoch 186/200, Train Loss: 0.1759\n",
      "Epoch 186/200, Validation Loss: 0.1433\n",
      "No improvement in validation loss for 8 epoch(s).\n",
      "Epoch 187/200, Train Loss: 0.1831\n",
      "Epoch 187/200, Validation Loss: 0.1926\n",
      "No improvement in validation loss for 9 epoch(s).\n",
      "Epoch 188/200, Train Loss: 0.1797\n",
      "Epoch 188/200, Validation Loss: 0.1407\n",
      "No improvement in validation loss for 10 epoch(s).\n",
      "Epoch 189/200, Train Loss: 0.1746\n",
      "Epoch 189/200, Validation Loss: 0.1347\n",
      "No improvement in validation loss for 11 epoch(s).\n",
      "Epoch 190/200, Train Loss: 0.1797\n",
      "Epoch 190/200, Validation Loss: 0.1615\n",
      "No improvement in validation loss for 12 epoch(s).\n",
      "Epoch 191/200, Train Loss: 0.1836\n",
      "Epoch 191/200, Validation Loss: 0.1412\n",
      "No improvement in validation loss for 13 epoch(s).\n",
      "Epoch 192/200, Train Loss: 0.1739\n",
      "Epoch 192/200, Validation Loss: 0.1570\n",
      "No improvement in validation loss for 14 epoch(s).\n",
      "Epoch 193/200, Train Loss: 0.1805\n",
      "Epoch 193/200, Validation Loss: 0.1325\n",
      "No improvement in validation loss for 15 epoch(s).\n",
      "Epoch 194/200, Train Loss: 0.1725\n",
      "Epoch 194/200, Validation Loss: 0.1236\n",
      "Best model updated based on validation loss.\n",
      "Epoch 195/200, Train Loss: 0.1831\n",
      "Epoch 195/200, Validation Loss: 0.1350\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 196/200, Train Loss: 0.1797\n",
      "Epoch 196/200, Validation Loss: 0.1720\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 197/200, Train Loss: 0.1778\n",
      "Epoch 197/200, Validation Loss: 0.1547\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 198/200, Train Loss: 0.1679\n",
      "Epoch 198/200, Validation Loss: 0.1288\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 199/200, Train Loss: 0.1709\n",
      "Epoch 199/200, Validation Loss: 0.1559\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Epoch 200/200, Train Loss: 0.1784\n",
      "Epoch 200/200, Validation Loss: 0.1927\n",
      "No improvement in validation loss for 6 epoch(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.31405437229194116,\n",
       "  0.2824251719045775,\n",
       "  0.27713258474526975,\n",
       "  0.27123617655650745,\n",
       "  0.272747484648409,\n",
       "  0.26825983718590024,\n",
       "  0.259887514175571,\n",
       "  0.2607160386675438,\n",
       "  0.26352551522816997,\n",
       "  0.2602383640237124,\n",
       "  0.2585733998743923,\n",
       "  0.26108243178307106,\n",
       "  0.25771783327705056,\n",
       "  0.25772924542058784,\n",
       "  0.25401831277338044,\n",
       "  0.25542200941719934,\n",
       "  0.25157794735249234,\n",
       "  0.25738655544902433,\n",
       "  0.2500154359988557,\n",
       "  0.25061152604171305,\n",
       "  0.2459478120199146,\n",
       "  0.25144450491499654,\n",
       "  0.24565141481974165,\n",
       "  0.24902454634398902,\n",
       "  0.2491461773944785,\n",
       "  0.24699034727401606,\n",
       "  0.24689068736600558,\n",
       "  0.24495353915873816,\n",
       "  0.24503811284503102,\n",
       "  0.24679691910375434,\n",
       "  0.23688724147998674,\n",
       "  0.2436384383886486,\n",
       "  0.2379405044883263,\n",
       "  0.24122077318708707,\n",
       "  0.23514887122543593,\n",
       "  0.23898196929159216,\n",
       "  0.23883086054837296,\n",
       "  0.2335222890997171,\n",
       "  0.23969638996364498,\n",
       "  0.2280672220336167,\n",
       "  0.23257495035477577,\n",
       "  0.2378746536471828,\n",
       "  0.23216933929520756,\n",
       "  0.22921390670702257,\n",
       "  0.2365490514678817,\n",
       "  0.23101299892785204,\n",
       "  0.2325885210632428,\n",
       "  0.23151395810385564,\n",
       "  0.2233069227926943,\n",
       "  0.22974840299130733,\n",
       "  0.22760734971286906,\n",
       "  0.22285588594160605,\n",
       "  0.22541831219179323,\n",
       "  0.21883070956873124,\n",
       "  0.21323905995703468,\n",
       "  0.21639603376211491,\n",
       "  0.21894934258207735,\n",
       "  0.21654158301240706,\n",
       "  0.21908242769270228,\n",
       "  0.21946498797675276,\n",
       "  0.22310619613956023,\n",
       "  0.21614046618979807,\n",
       "  0.21545744526196456,\n",
       "  0.21274742813596764,\n",
       "  0.2064336076725855,\n",
       "  0.20676864426894673,\n",
       "  0.20814072447935184,\n",
       "  0.20860270064274564,\n",
       "  0.20714813320607145,\n",
       "  0.2114247095137268,\n",
       "  0.20703175018573386,\n",
       "  0.20788822306794585,\n",
       "  0.20518590305035955,\n",
       "  0.23340017150577597,\n",
       "  0.2153720570260751,\n",
       "  0.2176467761213222,\n",
       "  0.21533240674120221,\n",
       "  0.21535348500206555,\n",
       "  0.20896422962867034,\n",
       "  0.21311535088154188,\n",
       "  0.20691043479870935,\n",
       "  0.20161742939411914,\n",
       "  0.20502769351253436,\n",
       "  0.20538293493968282,\n",
       "  0.206867812466967,\n",
       "  0.20749941493381,\n",
       "  0.20011899414553727,\n",
       "  0.2058957745692916,\n",
       "  0.2006927671950282,\n",
       "  0.19848361495216538,\n",
       "  0.20411919365095207,\n",
       "  0.1967226151100753,\n",
       "  0.20409189637559166,\n",
       "  0.20039740020140098,\n",
       "  0.1999027010083496,\n",
       "  0.20144016686083302,\n",
       "  0.1979135933966011,\n",
       "  0.2033635863818285,\n",
       "  0.19822934436162104,\n",
       "  0.19044079810926273,\n",
       "  0.19762681043944091,\n",
       "  0.19762537836012192,\n",
       "  0.19331633911854074,\n",
       "  0.19432750892800638,\n",
       "  0.1929674960403728,\n",
       "  0.1989985997646962,\n",
       "  0.19300057004761842,\n",
       "  0.1967508900714082,\n",
       "  0.1961409303912107,\n",
       "  0.18944272287048314,\n",
       "  0.19334239700539993,\n",
       "  0.2009020804032726,\n",
       "  0.1964682695002582,\n",
       "  0.18359876030298242,\n",
       "  0.19748242697483742,\n",
       "  0.19428482484568207,\n",
       "  0.19978763199917718,\n",
       "  0.19929289075230805,\n",
       "  0.18839204797039638,\n",
       "  0.1873966740716971,\n",
       "  0.19239026743621201,\n",
       "  0.19689713562254443,\n",
       "  0.19558083272748006,\n",
       "  0.19915496392048074,\n",
       "  0.18990123393049485,\n",
       "  0.1941830897756884,\n",
       "  0.18940997051478792,\n",
       "  0.19432654329876595,\n",
       "  0.18900535154836287,\n",
       "  0.18943928777569544,\n",
       "  0.18711153972192807,\n",
       "  0.1815161597897288,\n",
       "  0.19023441734189034,\n",
       "  0.1909780208377288,\n",
       "  0.19041796663542324,\n",
       "  0.18876205321079112,\n",
       "  0.1824614737756522,\n",
       "  0.1887064103369987,\n",
       "  0.192008231003404,\n",
       "  0.18690908929163116,\n",
       "  0.1880212088767085,\n",
       "  0.18371982843094564,\n",
       "  0.19606026958677733,\n",
       "  0.1872003556037952,\n",
       "  0.18276846446911363,\n",
       "  0.18288520426478788,\n",
       "  0.1878065198142889,\n",
       "  0.186375811577424,\n",
       "  0.1835007676960964,\n",
       "  0.18351072038279514,\n",
       "  0.18518000945604687,\n",
       "  0.18419639197861626,\n",
       "  0.1823228001842424,\n",
       "  0.18371733474747026,\n",
       "  0.1866018829615822,\n",
       "  0.1908177471411602,\n",
       "  0.18034184401847791,\n",
       "  0.18494337672339362,\n",
       "  0.18277908055317152,\n",
       "  0.18168528180059484,\n",
       "  0.1912508394246914,\n",
       "  0.18358836419165078,\n",
       "  0.1890458065977483,\n",
       "  0.18530827214651008,\n",
       "  0.1800878555761799,\n",
       "  0.19747832692281375,\n",
       "  0.17889540718103486,\n",
       "  0.17361537087708712,\n",
       "  0.17986439896690812,\n",
       "  0.1805954054892134,\n",
       "  0.1796743245087139,\n",
       "  0.1831358890792801,\n",
       "  0.1839881503561462,\n",
       "  0.18503624743148117,\n",
       "  0.18465900636090413,\n",
       "  0.179996683202089,\n",
       "  0.18540764391995937,\n",
       "  0.17770686810977843,\n",
       "  0.1833735182040332,\n",
       "  0.18124270086803476,\n",
       "  0.17699192519239487,\n",
       "  0.18215589702816348,\n",
       "  0.18058750652017588,\n",
       "  0.17700998401517198,\n",
       "  0.17242126405139954,\n",
       "  0.1758860592932189,\n",
       "  0.18308075420034362,\n",
       "  0.17973778054946785,\n",
       "  0.17457076534496074,\n",
       "  0.17970099197010095,\n",
       "  0.1836309621000783,\n",
       "  0.17390161638498702,\n",
       "  0.18048155834515908,\n",
       "  0.17247476611027898,\n",
       "  0.1830901338341699,\n",
       "  0.17969300440324917,\n",
       "  0.1777872869465146,\n",
       "  0.1678940148027295,\n",
       "  0.17094461005464368,\n",
       "  0.1783718347776072],\n",
       " [0.23522872444284096,\n",
       "  0.22445306543972385,\n",
       "  0.2276721645062894,\n",
       "  0.2088125938905424,\n",
       "  0.21377151792220186,\n",
       "  0.24553416826333221,\n",
       "  0.22481098773188135,\n",
       "  0.2143658600287156,\n",
       "  0.21171471888848234,\n",
       "  0.22064654562580452,\n",
       "  0.21548223279918846,\n",
       "  0.3261059438077252,\n",
       "  0.21023061220649253,\n",
       "  0.2117358289993881,\n",
       "  0.2018869531267647,\n",
       "  0.25321688256069513,\n",
       "  0.20327971155723829,\n",
       "  0.1979468780245339,\n",
       "  0.21034866347490402,\n",
       "  0.22276856346327936,\n",
       "  0.206274360324141,\n",
       "  0.19667504664008204,\n",
       "  0.20155777079000903,\n",
       "  0.20495374716316048,\n",
       "  0.1933125364751126,\n",
       "  0.2205374636216445,\n",
       "  0.22615443808392863,\n",
       "  0.20767236398428343,\n",
       "  0.2057694657130188,\n",
       "  0.22646695526128405,\n",
       "  0.24269724926084615,\n",
       "  0.20075284803725696,\n",
       "  0.21204067429715998,\n",
       "  0.1903885066237175,\n",
       "  0.18337121840273396,\n",
       "  0.2537508567774229,\n",
       "  0.1835433727214008,\n",
       "  0.18601675149513763,\n",
       "  0.1971517400816083,\n",
       "  0.19824268311010987,\n",
       "  0.18141071844762297,\n",
       "  0.1975509153804585,\n",
       "  0.18194598040093532,\n",
       "  0.18708627363352964,\n",
       "  0.18335455274104737,\n",
       "  0.2125709078933918,\n",
       "  0.27999622307801514,\n",
       "  0.1772406067950337,\n",
       "  0.18155594392983096,\n",
       "  0.1744824145826396,\n",
       "  0.17413326905349666,\n",
       "  0.1758353795458594,\n",
       "  0.17516235768627586,\n",
       "  0.16936101637822523,\n",
       "  0.1700343508378006,\n",
       "  0.17238254812179823,\n",
       "  0.2218189344249582,\n",
       "  0.17033406815836938,\n",
       "  0.2258117486921589,\n",
       "  0.1799660765545003,\n",
       "  0.1822009515121914,\n",
       "  0.25408071796462106,\n",
       "  0.1614113179077342,\n",
       "  0.1602668036403281,\n",
       "  0.16302829975094857,\n",
       "  0.15395130734534912,\n",
       "  0.16574642847009588,\n",
       "  0.1696693264314214,\n",
       "  0.16106388451050171,\n",
       "  0.25916676155260104,\n",
       "  0.15471223258842412,\n",
       "  0.18655163460421595,\n",
       "  0.15417519368733582,\n",
       "  0.17061836569140967,\n",
       "  0.174339314033225,\n",
       "  0.16957104870568165,\n",
       "  0.19396633255096635,\n",
       "  0.16211834162724822,\n",
       "  0.15916703030299606,\n",
       "  0.18093470877475953,\n",
       "  0.16174973505601453,\n",
       "  0.1592692675257332,\n",
       "  0.15025899749709648,\n",
       "  0.16435558430515648,\n",
       "  0.16696569617586524,\n",
       "  0.1556099240322796,\n",
       "  0.15755060049338956,\n",
       "  0.172513744783368,\n",
       "  0.14454211000604242,\n",
       "  0.1480102446087207,\n",
       "  0.14358357423811816,\n",
       "  0.1557331331220738,\n",
       "  0.16063977291891246,\n",
       "  0.15670245064401558,\n",
       "  0.15143311570994974,\n",
       "  0.14186665079336655,\n",
       "  0.14404045260940376,\n",
       "  0.14355699969141672,\n",
       "  0.14360543013435234,\n",
       "  0.1467598672495799,\n",
       "  0.14624762716127582,\n",
       "  0.1385178862641869,\n",
       "  0.17084696231765695,\n",
       "  0.13924680500594752,\n",
       "  0.15417239819182438,\n",
       "  0.14285389792299671,\n",
       "  0.14686776191163597,\n",
       "  0.14974211548779454,\n",
       "  0.16300582384502285,\n",
       "  0.19896538722527662,\n",
       "  0.14209026632977,\n",
       "  0.1436942805211698,\n",
       "  0.13848014800610503,\n",
       "  0.140079577751667,\n",
       "  0.13984962150956806,\n",
       "  0.1369729555274747,\n",
       "  0.17499001077219342,\n",
       "  0.1306418665381295,\n",
       "  0.13450528790106933,\n",
       "  0.1509264177653227,\n",
       "  0.17051440301571,\n",
       "  0.14471034464436802,\n",
       "  0.1441997957593772,\n",
       "  0.14448317092728247,\n",
       "  0.19104192314638097,\n",
       "  0.1435114455963956,\n",
       "  0.14515179193191483,\n",
       "  0.14187783447586083,\n",
       "  0.13761213687091564,\n",
       "  0.1498276874569527,\n",
       "  0.16243215986747253,\n",
       "  0.12921975782203876,\n",
       "  0.14250677512184287,\n",
       "  0.13590167218484403,\n",
       "  0.13907882570697183,\n",
       "  0.13666589494292322,\n",
       "  0.13214312921267715,\n",
       "  0.14735343353822827,\n",
       "  0.14305673223616702,\n",
       "  0.1356171877385023,\n",
       "  0.15077522505870025,\n",
       "  0.1437852161339997,\n",
       "  0.13137903634793638,\n",
       "  0.1439179495772284,\n",
       "  0.13139511791863637,\n",
       "  0.14182539305157876,\n",
       "  0.13840473959682864,\n",
       "  0.1317257218041949,\n",
       "  0.14992899763178122,\n",
       "  0.1290930829094618,\n",
       "  0.13403527801751755,\n",
       "  0.1321998693394276,\n",
       "  0.13023781120839917,\n",
       "  0.12766752721155794,\n",
       "  0.14582724455911455,\n",
       "  0.13328497794069602,\n",
       "  0.12749652563479175,\n",
       "  0.1387688027893643,\n",
       "  0.13171892292964993,\n",
       "  0.12735640515280322,\n",
       "  0.12928867480309492,\n",
       "  0.16660453363980973,\n",
       "  0.13711417078176577,\n",
       "  0.13716949651729357,\n",
       "  0.14126205956563354,\n",
       "  0.12731992260270406,\n",
       "  0.12758649417900303,\n",
       "  0.15102666489821806,\n",
       "  0.14909538935367647,\n",
       "  0.18984717028110884,\n",
       "  0.15738243695997287,\n",
       "  0.1372431354921604,\n",
       "  0.14151231797537608,\n",
       "  0.16066866442351863,\n",
       "  0.1349246749926484,\n",
       "  0.1489397234576388,\n",
       "  0.14354650200648086,\n",
       "  0.12408589734957459,\n",
       "  0.14744496290059236,\n",
       "  0.13020636552547135,\n",
       "  0.12776285685661637,\n",
       "  0.14092072202520592,\n",
       "  0.14504466869795088,\n",
       "  0.12540810308262204,\n",
       "  0.1411550866042295,\n",
       "  0.14325968061019195,\n",
       "  0.1925568965870594,\n",
       "  0.14068260126004226,\n",
       "  0.13470861361770148,\n",
       "  0.16146229873045106,\n",
       "  0.14121402248353018,\n",
       "  0.15700096538122954,\n",
       "  0.13254437913720526,\n",
       "  0.12356082377222817,\n",
       "  0.13496367822746547,\n",
       "  0.17199320459131445,\n",
       "  0.1547081857040692,\n",
       "  0.1288144209558207,\n",
       "  0.15594064743059238,\n",
       "  0.1927498665774304])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "b0dbb0009f51abf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:22.424652Z",
     "start_time": "2025-02-12T18:39:22.399987Z"
    }
   },
   "source": [
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load(model_checkpoints_path + \"/booster/\" + match + \".pth\", weights_only=True))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "a520e80c09e814c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:23.935357Z",
     "start_time": "2025-02-12T18:39:22.440173Z"
    }
   },
   "source": [
    "true_values, predicted_values, accuracy = evaluate(device, model, test_loader, model_checkpoints_path + \"/booster/\" + match + \".pth\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(12, 128, heads=1)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GATConv(128, 64, heads=1)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "Accuracy: 0.9445\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "810768846ddfc62a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:24.114144Z",
     "start_time": "2025-02-12T18:39:23.953544Z"
    }
   },
   "source": [
    "generate_matrix(true_values, predicted_values, accuracy, \"GAT Match: \"  + match)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "704d71a02da61822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:24.161697Z",
     "start_time": "2025-02-12T18:39:24.141152Z"
    }
   },
   "source": [
    "generate_metrics(true_values, predicted_values, match, model_checkpoints_path, NUM_EPOCHS, LEARNING_RATE, NODE_FEATURES, DROPOUT_RATE, PATIENCE, HIDDEN_DIM)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9130\n",
      "Recall: 0.9110\n",
      "F1 Score: 0.9120\n",
      "AUROC: 0.9355\n",
      "Accuracy: 0.9445\n",
      "Metrics saved to C:\\Users\\pavel\\PycharmProjects\\ADES-reliability\\src\\..\\models/booster/results/3-4.csv\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "f969cf1e-2578-4682-bf50-bb016798484c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:39:24.169076Z",
     "start_time": "2025-02-12T18:39:24.166705Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
